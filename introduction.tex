\section{Introduction}

\textit{It is now 10:00am in busy London. The kidney transplant department in St Thomas Hospital (Hospital A) has been informed that a matching kidney of a patient in Guy's Hospital (Hospital B) is now available for its patient waiting for surgery. The organ transplant department of both hospitals contact the SOSDronePayload company and make all the necessary arrangements for the organ to be transferred by drone from Hospital B to Hospital A. Drone\_DR1235 is selected by SOSDronePayload to deliver the organ through its flying corridor over the Thames River, to avoid land traffic. At a certain point during the journey the battery of Drone\_DR1235 reduces dramatically and when it reaches 10\%, Drone\_DR1235 attempts to land before reaching its final destination (Hospital A). However, at this point, Drone\_DR1235 was only 2 km away from Hospital A, 
% wind strong
and given the favourable wind conditions at the time, 
Drone\_DR1235 could have made the journey to Hospital A with its 10\% battery capacity and delivered the life critical organ.}

The above scenario is not unrealistic and likely to become increasingly common. In the scenario, \textit{Drone\_DR1235} was  developed independently of the payload organ delivery application, and was not intended to change its behaviour during its execution, regardless of the application in which it is used. 

Nowadays, many applications are developed by the composition of independently created software components into a single system. These systems are known as systems-of-systems~\cite{Maier98SOS} and allow the achievement of certain functionalities that cannot be achieved by individual participating components on their own.  In such situations, it is necessary to support emergent behaviours that appear due to the combination of existing components into new larger systems, or even due to new requirements or context changes ~\cite{SilvaSouza:2011:ARA:1988008.1988018}. The participating software components, however, have been designed to satisfy predefined requirements following predefined specifications and are not necessarily intended to change their behaviour during execution, nor to support some global requirements of the systems-of-systems applications. We refer to components that resist such changes, despite the need, as {\it defiant} components. 

Existing adaptation approaches have not considered such defiant components~\cite{filieri2011, DBLP:conf/dagstuhl/LemosGGGALSWBBB13, szvetits:2016, barbosa:2017, arcaini:2017}. In these approaches, models are often specified based on finite-state machine formalisms such as a labelled transition systems (LTS)~\cite{Magee:2006:CSM:1076396} or Markov chains~\cite{KNP11}, to represent system behaviour, verify property violation, and check the correctness of the adaptation. However, dealing with these models requires significant mathematical skills from the software engineers, which may hinder their adoption by users~\cite{barbosa:2017}. Such behaviour models also require a complete description of the system behaviour for a fixed scope and span~\cite{uchitel:2013}, which may not be possible for adaptive systems since not all adaptive behaviours can be modelled in advance due to unexpected requirements or environment uncertainty. 
 
In this paper we suggest a novel approach that considers the existence of defiant components. More specifically, we present a {\it cautious adaptation} approach in which changes in the behaviour of the components are acccomodated, in order to satisfy global requirements in exceptional circumstances. The adaptation is {\it cautious} as it guarantees that changes will not interfere with the original functionality of the participating components during their normal use, and will only be triggered in  exceptional conditions. For example, in the above scenario, the drone is considered a {\it defiant} component since it has been developed with a specification in which it should make a safe landing when its battery reaches 10\%. However, from the perspective of the payload organ delivery application, it should be possible to force the drone to continue flying, given the exceptional and favourable wind conditions, which will allow the drone to complete its journey.

Our approach relies on the use of {\it wrappers}, implemented using an aspect-oriented programming technique~\cite{Kiczales:2001}, to support exceptional conditions that are formalised with behavioural semantics using LTS~\cite{Magee:2006:CSM:1076396}. In our approach, exceptional conditions are represented by join-points of scenario-based aspect weaving~\cite{Xu07}, given that they can handle exceptions. The use of scenarios to identify exceptional conditions has been demonstrated successfully  in the literature~\cite{Young‚ÄêGab:2011}\cite{Ban:2014}, and used to support behavioural model checking~\cite{uchitel:2003}. The high abstraction level provided by scenarios facilitates identification and understanding of requirements and involvement of different stakeholders in an application. The use of scenarios provides a framework for analysing future events of an application, by considering alternative situations, and helps with decision-making. 

The use of aspect-oriented programming  provides wrappers to support introducing changes into defiant components without requiring consent from the designer of the components. It avoids the need to redesign a component to satisfy emergent behaviours, as in the case when using, say,  dependency injection techniques~\cite{fowlerioc}  or plugin-based approaches~\cite{Wermelinger:2008:AEE:1370750.1370783}. On the other hand, the use of aspects-oriented programming techniques can be risky, since changes introduced by the use of aspects may violate the original (local) requirements of the components. Our work uses aspect-oriented techniques in a way that guarantees the original requirements of the components under normal execution conditions.  

The remainder of the paper is structured as follows. Section 2 describes our approach to support cautious adaptation of defiant components. Section 3 presents a running example of the payload organ delivery application. Section 4  discusses some conclusions and pointers to future work.



% Example
%Consider a system consists of Internet of Things (IoT) as an example. A person is running on an exercise machine and wearing a device that monitors the heartbeat rates. The person incrementally speeds up the machine, trying to test the limit. At a certain point, the wearable device alerts the person that the heart is beating very fast and something bad could happen. However, the runner does not listen to the advice and keeps increasing the machine speed (and, indirectly, his/her heart beating rate), thus putting the life in danger. Aware of the hazard, the wearable device connects to the training machine and stops it, against the runner's will.

%Consider an off-the-shelf drone that is designed to land safely when it cannot fly properly due to situations such as short of battery. This is a safety feature of commodity product where the software logic embedded in the controller firmware has been designed to satisfy a safety case properly. It is no longer safe any more, however, when the drone is being used for delivering a payload through the sky above water -- a legitimate medical use case to avoid traffic congestion in Central London when delivering emergency supply of blood and organs through a flight corridor on the Thames river. In the system perspective landing a drone on water is safer than landing it on someone's head or on a fast moving vehicle.

%For the drone flying above the river, the ``safe landing feature'' would drop the drone into water, losing a chance to achieve the system goal, i.e. to deliver the assets timely, because the sunk drone and payloads would require extra time to rescue. 

%In fact, the situation when it happens may not be all that pessimistic. If there is a bless of wind to add to the speed of
%the drone, and the remaining estimated time of arrival (ETA) is shorter than how long the drone can fly, a better recommendation for the drone is to fly on, overriding the internal condition to ``safe landing'' instruction set by the defiant component.

%While flying, the drone reports periodically to the pilot about its environment and internal status such as GPS locations and its battery levels. Suppose that the drone reports that its battery level is low and that it will not be able to land in the predefined place. Due to some internal failure or to an external interference (e.g., long distance from the pilot or bad weather conditions), the low-battery message does not reach the pilot, who keeps instructing the drone to fly forward. In this situation, the drone decides by itself to do a safe landing, since an out-of-battery failure could cause a  serious accident. However, as it is flying over a river (suppose the only allowed flight corridor in a big urban city), the drone may try to contact a ferry boat to rescue it, which has to decide whether it can help considering the distance and time of the probable drone landing place. In both examples, one of the components defies, either deliberately (e.g., the runner in the first example) or unintentionally (e.g., the pilot in the second example), to obey or accept messages from other component, thus putting at risk parts of the system and its surrounding environment. 

% Generalise the problem
%Although we have only described two possible scenarios, 
%The {\em defiant} problem may happen to various applications including cyber-physical systems, service-based systems, cyber security, system of systems, and cloud computing. The problem is challenging to predict at design time since each component has only local awareness of the system, restricted to its own behaviour model. However, when the components interact with each other at runtime, a global awareness of the system is needed where the interaction among the components cannot be anticipated at design time, due to the unpredictable human behaviour, possible component failures or on-the-fly intercommunication between a highly inter-operable and decoupled systems. Furthermore, the decisions about system conflict resolutions have been taken at design time.     

%% Solution requirements
%The solution to this ``defiant component'' problem needs a new type of software analysis approach, we call ``cautious adaptation'', wherein a {\em wrapper} of the defiant component is added so that (1) when the composed component does not exhibit defiant behaviour any more, without changing (tolerating) the defiant component at all; (2) the system shall satisfy the global safety goal, while relaxing the safety requirement of the defiant component in the exceptional scenarios. 

%% A solution
%What are our contributions
%As a contribution, we expect to improve the understanding of information modelling in runtime adaptive systems...  
%Given safety relaxation requirements and exceptional scenarios,
%there might be several ways to design the cautious adaptation wrappers. 

%In this paper, we propose one way to address this problem, by composing the defiant component and the wrapper through {\em decentralised dependency injection}. 

%% why dependency injection
%The proposed dependency injection inverses the control of dependencies between the defiant component and the wrapper function to satisfy the global goal in a situation that the component was not anticipating. Such cautious adaptation takes advantage of the situation awareness only known at the system level.

%% why decentralised
%Furthermore, the proposed solution is decentralised by design, because the wrapper may not be a single component. Indeed when dependence injector prepares the instantiation of the wrapper component incrementally, it is possible to distribute the wrapper functionality to several components. 

%% why cautious
%Built on top of established scenario-driven approach to behavioural model checking~\cite{uchitel2003}, we also introduce an exceptional scenario-driven approach to provide the needed assurance through model checking the {\em weakest safety property} of a defiant component. If such an optimistic assurance is not feasible, we degenerate the solution to a failure-responsive mode to reduce the risk to break an existing safety case which already governs the design of the defiant component.

%decides that the non-defiant component has to take over the system action control and try to leads the system to safe state. Such solution could have save real lives if, for instance, the plane that carried the Brazilian football team Chapecoense would have this solution implemented and, although the pilot insisted in keep flying, the airplane system would land it safely in the near airport, thus not running out of fuel.
%
%Some work have already proposed approaches for the dynamic composition of components, either in the cloud~\cite{Mathlouthi:2017}, in a service-based (eco)systems~\cite{Sun:2006}\cite{Cervantes:2014}, system of systems~\cite{Ricci:2012}, pervasive systems~\cite{Cervantes:2018}, they usually focus on engineering such composition or proposing new protocols, taking into account some non-functional requirements, such as performance, but do not address new safety requirements that arise for the new composed global system.   
%


% What are the challenges about this problem
%For that situation, some challenges arise as, for instance, how to model the system such that an early analysis of the risks can be carried out and, at the same time, propose a runtime adaptation to solve the conflict? For the best of our knowledge, no such approach for modelling the adaptation of the defiant component has been proposed.  

%Generally, to address behaviour adaptation, models are specified based on a finite-state machine formalism, such as a Labelled Transition System or Markov Chains, to represent the system behavior, verify property violation, and check the correctness of the adaptation\cite{filieri2011}\cite{bencomo2013}\cite{szvetits2016}\cite{barbosa2017}\cite{arcaini2017}. However, dealing with that kind of model demands a solid mathematical background by software engineers, which may hinder the adoption of such approaches for users who are not familiar with formal methods~\cite{barbosa2017}. Moreover, using such behaviour models requires a complete description of the system behaviour for a fixed scope and span~\cite{uchitel2013}, which may not be possible for a self-adaptive system since not all possible adaptive behaviours be modelled in advance due to unexpected changing requirements or environment uncertainty. 

% literature on scenarios
%In this realm, scenarios emerge as an alternative to model the expected (design time) and desired (run time) behaviours~\cite{uchitel2003}. Scenarios are a widespread modelling technique that, due to their high level of abstraction, eases the comprehension of requirements and the involvement of different stakeholders and provides a framework for analysing possible future events by considering alternative possible situations, thus helping the decision-making task. A scenario often refers to a partial description of the interaction between users, system components, and the environment in the restricted context of achieving some implicit purpose(s). The term \textit{partial} means that scenarios decompose complex system behaviour into smaller ``stories'' that are easy to understand and that, when combined with other scenarios, provide an overall system description. Each partial story typically focuses on different system functionalities

%More specifically, scenarios have been used successfully for security risk analysis and evaluation~\cite{Young‚ÄêGab:2011}\cite{Ban:2014}.  

% what is our solution proposal
% why decentralized?
%In this paper, we propose a scenario-driven adaptation approach to model and resolve runtime conflicts regarding a defiant component. We use a Message Sequence Chart to model the scenarios and extend them with a special operator to indicate when a problem can take place. 


